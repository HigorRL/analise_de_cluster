# -*- coding: utf-8 -*-
"""Análise_de_Cluster_2305

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T9s-FnTOxJ8mREneiW9N8NM9lzO53JsO
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

dataset = pd.read_csv('Mall_Customers.csv')

dataset.head()

X = dataset[['Annual Income (k$)', 'Spending Score (1-100)']]

X.head()

import scipy.cluster.hierarchy as sch

dendrograma = sch.dendrogram(sch.linkage(X, method = 'ward'))
plt.title('Dendrograma')
plt.xlabel('Clientes')
plt.ylabel('Distâncias Euclidianas')
plt.show()

from sklearn.cluster import AgglomerativeClustering

hc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'ward')
y_hc = hc.fit_predict(X)

y_hc

plt.figure(1, figsize=(15,7))
plt.scatter(X[y_hc == 0]['Annual Income (k$)'], X[y_hc == 0]['Spending Score (1-100)'],
            s=200, c='red', label = 'Cluster1')
plt.scatter(X[y_hc == 1]['Annual Income (k$)'], X[y_hc == 1]['Spending Score (1-100)'],
            s=200, c='blue', label = 'Cluster2')
plt.scatter(X[y_hc == 2]['Annual Income (k$)'], X[y_hc == 2]['Spending Score (1-100)'],
            s=200, c='green', label = 'Cluster3')
plt.scatter(X[y_hc == 3]['Annual Income (k$)'], X[y_hc == 3]['Spending Score (1-100)'],
            s=200, c='cyan', label = 'Cluster4')
plt.scatter(X[y_hc == 4]['Annual Income (k$)'], X[y_hc == 4]['Spending Score (1-100)'],
            s=200, c='magenta', label = 'Cluster5')
plt.title('Clusters do Método de Ward')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.show()

"""### KMeans"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
import os

df = pd.read_csv('Mall_Customers.csv')

df.head()

from pandas.core.series import algorithms
X2 = df[['Annual Income (k$)', 'Spending Score (1-100)']]
inertia = []
for n in range(1, 11):
  algorithm = (KMeans(n_clusters = n))
  algorithm.fit(X2)
  inertia.append(algorithm.inertia_)

plt.figure(1, figsize = (15, 6))
plt.plot(np.arange(1, 11), inertia, 'o')
plt.plot(np.arange(1, 11), inertia, '-', alpha=0.5)
plt.xlabel('Número de Clusters')
plt.ylabel('Soma das Distancias Q intra Clusters')
plt.show()

algorithm = (KMeans(n_clusters = 5))
algorithm.fit(X2)

labels2 = algorithm.labels_
plt.figure(1, figsize = (15, 7))
plt.scatter(x='Annual Income (k$)', y = 'Spending Score (1-100)', data= df,
            c = labels2, s = 200)
plt.ylabel('Spending Score (1-100)')
plt.xlabel('Annual Income (k$)')
plt.show()

"""### Vinhos - Ward"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.datasets import load_wine
from sklearn.preprocessing import LabelEncoder

wine = load_wine()

X = pd.DataFrame(wine.data, columns=wine.feature_names)
Y = pd.Categorical.from_codes(wine.target, wine.target_names)

df = X.join(pd.Series(Y, name='class'))
le = LabelEncoder()
y = le.fit_transform(df['class'])

lda = LinearDiscriminantAnalysis()
X_lda = lda.fit_transform(X, y)

dataFrameX = pd.DataFrame(X_lda, columns=['X', 'y'])

dendrograma = sch.dendrogram(sch.linkage(X, method= 'ward'))
plt.title('Dendrograma')
plt.xlabel('X')
plt.ylabel('y')
plt.show()

hc = AgglomerativeClustering(n_clusters = 3, affinity = 'euclidean', linkage = 'ward')
y_hc = hc.fit_predict(X)

plt.figure(1, figsize=(15,7))
plt.scatter(dataFrameX[y_hc == 0]['X'], dataFrameX[y_hc == 0]['y'],
            s=200, c='red', label = 'Cluster1')
plt.scatter(dataFrameX[y_hc == 1]['X'], dataFrameX[y_hc == 1]['y'],
            s=200, c='blue', label = 'Cluster2')
plt.scatter(dataFrameX[y_hc == 2]['X'], dataFrameX[y_hc == 2]['y'],
            s=200, c='green', label = 'Cluster3')
plt.title('Clusters do Método de Ward')
plt.xlabel('X')
plt.ylabel('Y')
plt.show()

"""###Vinhos - KMeans"""

inertia = []
for n in range(1, 11):
  algorithm = (KMeans(n_clusters = n))
  algorithm.fit(X)
  inertia.append(algorithm.inertia_)

plt.figure(1, figsize = (15, 6))
plt.plot(np.arange(1, 11), inertia, 'o')
plt.plot(np.arange(1, 11), inertia, '-', alpha=0.5)
plt.xlabel('Número de Clusters')
plt.ylabel('Soma das Distancias Q intra Clusters')
plt.show()

algorithm = (KMeans(n_clusters = 2))
algorithm.fit(X)

labels2 = algorithm.labels_
plt.figure(1, figsize = (15, 7))
plt.scatter(x='X', y = 'y', data= dataFrameX,
            c = algorithm.labels_, s = 200)
plt.ylabel('X')
plt.xlabel('Y')
plt.show()

